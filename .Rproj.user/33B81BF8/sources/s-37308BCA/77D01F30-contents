library(parallel)
library(mvtnorm)
library(clusterGeneration)
library(Matrix)
library(ks)
library(bbemkr)
set.seed(1001)

# a function to find the NP threshold order
np_order_stat <- function(size, alpha, delta) {
	violation_rates <- pbinom(q=0:(size-1), size=size, prob=1-alpha, lower.tail=F) 
	return( which(violation_rates <= delta)[1] )
}

# a function to calculation the sample-level classical criterion and NP criteria corresponding to varying alphas
crit <- function(x, y, B, alpha_s, delta, multi.core=F) {
	# x: an nxp feature matrix, with every column standarized
	# y: an n-dim 0/1 response vector
	# B: number of random splits
	# alpha_s: a vector of alpha values
	# delta: the tolerance level
	set.seed(1001)
	p <- ncol(x)
	n <- nrow(x)
	idx0 <- which(y == 0)
	idx1 <- which(y == 1)
	n0 <- length(idx0)
	n1 <- length(idx1)
	x0 <- x[idx0,]
	x1 <- x[idx1,]
	
	# for the Nadaraya-Watson approach, find a bandwidth h by five-fold cross-validation
	# fold_idx <- sample(1:5, size=n, replace=TRUE)
	# fold_idx <- lapply(1:5, FUN=function(k) which(fold_idx==k))
	# h_s <- seq(0.5, 2, by=0.5)
	# h_s <- unlist( mclapply(1:p, FUN=function(j) {
	# 	h_s_cverr <- sapply(h_s, FUN=function(h) {
	# 		mean( sapply(1:5, FUN=function(k) {
	# 			test_idx <- fold_idx[[k]]
	# 			train_idx <- setdiff(1:n, test_idx)
	# 			res <- NadarayaWatsonkernel(x[train_idx, j], y[train_idx], h = h, gridpoint = x[test_idx, j])
	# 			y_test_hat <- as.numeric( res$mh > 0.5 )
	# 			sum((y[test_idx] - y_test_hat) != 0) / length(test_idx)
	# 		}) )
	# 	})
	# 	h_s[which.min(h_s_cverr)]
	# }, mc.cores=detectCores()-1) )
	
	if (multi.core) {
		res <- mclapply(1:B, FUN=function(b) {
			n0_ts <- ceiling(n0/2)
			n0_lo <- n0 - n0_ts
			idx0_ts <- sample(1:n0, n0_ts)
			x0_ts <- x0[idx0_ts,]
			x0_lo <- x0[-idx0_ts,]
		
			n1_ts <- ceiling(n1/2)
			n1_lo <- n1 - n1_ts
			idx1_ts <- sample(1:n1, n1_ts)
			x1_ts <- x1[idx1_ts,]
			x1_lo <- x1[-idx1_ts,]
		
			x_ts <- rbind(x0_ts, x1_ts)
			y_ts <- c(rep(0, n0_ts), rep(1, n1_ts))
		
			x_lo <- rbind(x0_lo, x1_lo)
			y_lo <- c(rep(0, n0_lo), rep(1, n1_lo))
			n_lo <- n0_lo + n1_lo
		
			sapply(1:p, FUN=function(j) {
				################### KDE ratio approach ######################
				# for the j-th feature, construct kde functions on the train-scoring data and evaluate the function at the left-out data
				d0 <- kde(x=x0_ts[,j], eval.points=x_lo[,j])$estimate
				d0[is.na(d0) | d0<=0] <- 10^(-88)
				d1 <- kde(x=x1_ts[,j], eval.points=x_lo[,j])$estimate
				d1[is.na(d1) | d1<=0] <- 10^(-88)
				# classification scores on the left-out data
				s <- d1 / d0
				s0 <- s[1:n0_lo]
				s1 <- s[(n0_lo+1):n_lo]
				## classical criterion
				t_cl <- n0_ts/n1_ts
				y_lo_hat <- as.numeric( s > t_cl )
				R_kde <- sum((y_lo - y_lo_hat) != 0) / n_lo
				## NP criterion
				# classification scores on the left-out class 0 sample
				t <- s0
				t <- sort(t)
				# type II errors corresponding to varying alphas
				R1_kde_s <- sapply(alpha_s, FUN=function(alpha) {
					# thresholds
					t_np <- t[np_order_stat(n0_lo, alpha, delta)]
					# type II error
					sum(s1 <= t_np) / n1_lo
				})
			
				# ################### Nadaraya-Watson approach ######################
				# # for the j-th feature, construct Nadaraya-Watson estimators on the train-scoring data and evaluate the function at the left-out data; use the bandwidths in h_s
				# s <- NadarayaWatsonkernel(x_ts[,j], y_ts, h = h_s[j], gridpoint = x_lo[,j])$mh
				# s0 <- s[1:n0_lo]
				# s1 <- s[(n0_lo+1):n_lo]
				# ## classical criterion
				# y_lo_hat <- as.numeric( s > 0.5 )
				# R_nw <- sum((y_lo - y_lo_hat) != 0) / n_lo
				# ## NP criterion
				# # classification scores on the left-out class 0 sample
				# t <- s0
				# t <- sort(t)
				# # type II errors corresponding to varying alphas
				# R1_nw_s <- sapply(alpha_s, FUN=function(alpha) {
				# 	# thresholds
				# 	t_np <- t[np_order_stat(n0_lo, alpha, delta)]
				# 	# type II error
				# 	sum(s1 <= t_np) / n1_lo
				# })
				# return(list(R_kde=R_kde, R1_kde_s=R1_kde_s, R_nw=R_nw, R1_nw_s=R1_nw_s))
				return(c(R_kde, R1_kde_s))
			})
		}, mc.cores=detectCores()-1)
	} else {
		res <- lapply(1:B, FUN=function(b) {
			n0_ts <- ceiling(n0/2)
			n0_lo <- n0 - n0_ts
			idx0_ts <- sample(1:n0, n0_ts)
			x0_ts <- x0[idx0_ts,]
			x0_lo <- x0[-idx0_ts,]
		
			n1_ts <- ceiling(n1/2)
			n1_lo <- n1 - n1_ts
			idx1_ts <- sample(1:n1, n1_ts)
			x1_ts <- x1[idx1_ts,]
			x1_lo <- x1[-idx1_ts,]
		
			x_ts <- rbind(x0_ts, x1_ts)
			y_ts <- c(rep(0, n0_ts), rep(1, n1_ts))
		
			x_lo <- rbind(x0_lo, x1_lo)
			y_lo <- c(rep(0, n0_lo), rep(1, n1_lo))
			n_lo <- n0_lo + n1_lo
		
			sapply(1:p, FUN=function(j) {
				################### KDE ratio approach ######################
				# for the j-th feature, construct kde functions on the train-scoring data and evaluate the function at the left-out data
				d0 <- kde(x=x0_ts[,j], eval.points=x_lo[,j])$estimate
				d0[is.na(d0) | d0<=0] <- 10^(-88)
				d1 <- kde(x=x1_ts[,j], eval.points=x_lo[,j])$estimate
				d1[is.na(d1) | d1<=0] <- 10^(-88)
				# classification scores on the left-out data
				s <- d1 / d0
				s0 <- s[1:n0_lo]
				s1 <- s[(n0_lo+1):n_lo]
				## classical criterion
				t_cl <- n0_ts/n1_ts
				y_lo_hat <- as.numeric( s > t_cl )
				R_kde <- sum((y_lo - y_lo_hat) != 0) / n_lo
				## NP criterion
				# classification scores on the left-out class 0 sample
				t <- s0
				t <- sort(t)
				# type II errors corresponding to varying alphas
				R1_kde_s <- sapply(alpha_s, FUN=function(alpha) {
					# thresholds
					t_np <- t[np_order_stat(n0_lo, alpha, delta)]
					# type II error
					sum(s1 <= t_np) / n1_lo
				})
			
				# ################### Nadaraya-Watson approach ######################
				# # for the j-th feature, construct Nadaraya-Watson estimators on the train-scoring data and evaluate the function at the left-out data; use the bandwidths in h_s
				# s <- NadarayaWatsonkernel(x_ts[,j], y_ts, h = h_s[j], gridpoint = x_lo[,j])$mh
				# s0 <- s[1:n0_lo]
				# s1 <- s[(n0_lo+1):n_lo]
				# ## classical criterion
				# y_lo_hat <- as.numeric( s > 0.5 )
				# R_nw <- sum((y_lo - y_lo_hat) != 0) / n_lo
				# ## NP criterion
				# # classification scores on the left-out class 0 sample
				# t <- s0
				# t <- sort(t)
				# # type II errors corresponding to varying alphas
				# R1_nw_s <- sapply(alpha_s, FUN=function(alpha) {
				# 	# thresholds
				# 	t_np <- t[np_order_stat(n0_lo, alpha, delta)]
				# 	# type II error
				# 	sum(s1 <= t_np) / n1_lo
				# })
				# return(list(R_kde=R_kde, R1_kde_s=R1_kde_s, R_nw=R_nw, R1_nw_s=R1_nw_s))
				return(c(R_kde, R1_kde_s))
			})
		})
	}
	for (b0 in 1:B) {
		tmp <- res[[b0]]
		if (is.numeric(tmp)) {
			break
		}
	}
	count_tmp <- 1
	if (b0 < B) {
		for (b in (b0+1):B) {
			if (is.numeric(res[[b]])) {
				tmp <- tmp + res[[b]]
				count_tmp <- count_tmp + 1
			}		
		}
	}
	return(tmp/count_tmp)
}

# a function to calculation the population-level classical criterion and NP criteria corresponding to varying alphas when the two class-conditional distributions are Gaussian
crit_gauss <- function(x_huge, y_huge, p, mu_0, mu_1, Sigma_0, Sigma_1, alpha_s) {
	# x_huge: a huge dataset generated from the population: x
	# y_huge: a huge dataset generated from the population: y
	# p: P(Y=1)
	# mu_0: E[X|Y=0]
	# mu_1: E[X|Y=1]
	## Assume that the mean of class 0 <= the mean of class 1 for every feature
	# Sigma_0: var[X|Y=0]
	# Sigma_1: var[X|Y=1]
	# alpha_s: a vector of alpha values for the NP criterion
	d <- length(mu_0) # the dimension of X
	sapply(1:d, FUN=function(j) {
		mu_0j <- mu_0[j]
		mu_1j <- mu_1[j]
		sigma_0j <- sqrt(Sigma_0[j,j])
		sigma_1j <- sqrt(Sigma_1[j,j])
		print(j)
		# classical criterion
		# a function to compute the oracle one-dimensional classifier
		classical_rule <- function(x) {
			# based on X ~ two one-dimensional Gaussians
			# mu1 <- mu_0j
			# mu2 <- mu_1j
			# Sigma1 <- sigma_0j^2
			# Sigma2 <- sigma_1j^2
			# pi1 <- 1-p
			# pi2 <- p
			#
			# mu <- (mu1 + mu2)/2
			# Omega <- 1/Sigma2 - 1/Sigma1
			# delta <- (1/Sigma1 + 1/Sigma2) * (mu1 - mu2)
			# eta <- 2*log(pi1/pi2) + (1/4)*(mu1-mu2)^2*Omega + log(Sigma2) - log(Sigma1)
			#
			# bayes_discri_rule <- (x-mu)^2*Omega + delta*(x-mu) + eta
			
			bayes_discri_rule <- p * dnorm(x, mean=mu_1j, sd=sigma_1j) - (1-p) * dnorm(x, mean=mu_0j, sd=sigma_0j)
			return(as.numeric(bayes_discri_rule > 0))
		}
		y_hat <- classical_rule(x_huge[,j])
		R <- sum( (y_hat - y_huge)!=0 ) / length(y_huge)
		# NP criterion
		NP_thre_s <- qnorm(alpha_s, mean=mu_0j, sd=sigma_0j, lower.tail=FALSE)
		R1_s <- pnorm(NP_thre_s, mean=mu_1j, sd=sigma_1j, lower.tail=TRUE)
		return(c(R, R1_s))
	})
}

# a function to calculate the agreement between a feature-rank list and the true feature-rank list
top_k_acc <- function(rank_list, true_rank_list) {
	# rank_list and true_rank_list should have the same length
	n <- length(rank_list)
	n_true <- length(true_rank_list)
	if (n != n_true) {
		stop("The rank lists must have the same length")
	}
	sapply(1:n, FUN=function(k) {
		length(intersect(rank_list[1:k], true_rank_list[1:k])) / k
	})	
}

# a function to calculate the consistency across more than one feature-rank lists
top_k_consist <- function(list_of_rank_lists) {
	# all rank lists should have the same length
	n_s <- sapply(list_of_rank_lists, length)
	n <- unique(n_s)
	if (length(n) > 1) {
		stop("The rank lists must have the same length")
	}
	sapply(1:n, FUN=function(k) {
		tmp <- list_of_rank_lists[[1]][1:k]
		for (i in 2:length(list_of_rank_lists)) {
			tmp <- intersect(tmp, list_of_rank_lists[[i]][1:k])
		}
		length(tmp) / k
	})
}

# a function to convert values into ranks (rank=1 is the minimum)
val_to_rank <- function(vals) {
	n <- length(vals)
	ranks <- 1:n
	return(ranks[order(vals)])
}